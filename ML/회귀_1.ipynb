{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLA4fKuc/DTiu3fhzIiVaQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###1. 회귀\n",
        "\n",
        "#### **회귀** : 여러 개의 독립 변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법\n",
        "\n",
        "-> 머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 **최적의 회귀계수**를 찾아내는 것\n",
        "\n",
        "#### 회귀 유형\n",
        "\n",
        "  (1) 단일 회귀 : 독립변수 개수 **1개**\n",
        "\n",
        "  (2) 다중 회귀 : 독립변수 개수 **여러 개**\n",
        "\n",
        "  (3) 선형 회귀 : 회귀 계수의 결합이 **선형**\n",
        "\n",
        "  (4) 비선형 회귀 : 회귀 계수의 결합이 **비선형**\n",
        "\n",
        "#### 지도학습 유형\n",
        "\n",
        "(1) Classification : 예측값이 **이산형** 클래스\n",
        "\n",
        "(2) Regression : 예측값이 **연속형** 숫자값\n",
        "\n",
        "#### 대표적인 선형 회귀 모델\n",
        "\n",
        "*규제 : 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 페널티 값을 적용하는 것* \n",
        "\n",
        "* 일반선형회귀 : 예측값과 실제 값의 **RSS를 최소화**할 수 있도록 최적화, 규제 X\n",
        "* 릿지(RIdge) : 선형 회귀에 **L2 규제**를 추가한 회귀 모델\n",
        "  * `L2` 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해 회귀 계수값을 더 작게 만드는 규제 모델\n",
        "* 라쏘(Lasso) : 선형 회귀에 **L1 규제**를 적용한 방식\n",
        "  * `L1(=피처 선택 기능)` 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 선택되지 않게 함\n",
        "* 엘라스틱넷(ElasticNet) : **L2,L1**규제를 함께 결합한 모델로 주로 피처가 많은 데이터세트에서 적용\n",
        "* 로지스틱 회귀(Logistic Regression) : **분류**에 사용되는 선형 모델"
      ],
      "metadata": {
        "id": "xj0qfoX-KxBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 단순 선형 회귀를 통한 회귀 이해\n",
        "\n",
        "### 1) 회귀식\n",
        "## $$\\hat{Y} = w_0 + w_1 * X$$\n",
        "\n",
        "* $w_1$ , $w_0$ : 회귀 계수\n",
        "* 잔차 : 실제 값과 회귀 모델의 차이에 따른 오류값\n",
        "\n",
        "### 2) RSS(Residual Sum of Square)\n",
        "\n",
        "##$$RSS(w_0,w_1 )= {1 \\over N} \\sum\\limits_{i=1}^N{(y_i -(w_0 + w_1 *x_i)^2}$$\n",
        "\n",
        "* 회귀식의 독립변수 X, 종속변수 Y가 중심변수가 아니라 **회귀계수가** 중심 변수"
      ],
      "metadata": {
        "id": "m_apJrJjNV9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 비용 최소화하기 - 경사 하강법(Gradient Descent) 소개\n",
        " : 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트 하면서 오류 값이 최소가 되는 **W 파라미터 구하는 방식**\n",
        "\n",
        "**Step1** $w_0,w_1$를 임의의 값으로 설정하고 첫 비용 함수의 값 계산\n",
        "\n",
        "**Step2** 다음과 같이 업데이트 한 후 다시 비용함수의 값을 계산\n",
        "\n",
        "$w_1 = w_1 + n{2 \\over N}\\sum\\limits_{i=1}^Nx_i *(실제값-예측값)$,\n",
        "$w_0 = w_0 + n{2 \\over N}\\sum\\limits_{i=1}^Nx_i *(실제값-예측값)$\n",
        "\n",
        "**Step3** 비용 함수의 값이 감소했다면 Step2 반복, 더 이상 값이 감소하지 않으면 반복 중지"
      ],
      "metadata": {
        "id": "KiQ0RBA0NplE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(0)\n",
        "X = 2*np.random.rand(100,1)\n",
        "y = 6 + 4* X+np.random.randn(100,1)\n",
        "\n",
        "plt.scatter(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "o5yrbuHvNbLD",
        "outputId": "b5ba2f5d-b5e5-41a5-e8a8-e2da77cd8725"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7effdd81ce10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#비용 함수 정의\n",
        "\n",
        "def get_cost(y,y_pred) :\n",
        "  N = len(y)\n",
        "  cost = np.sum(np.square(y-y_pred))/N\n",
        "  return cost"
      ],
      "metadata": {
        "id": "d5yNrUE2NvHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#경사하강법 구현\n",
        "\n",
        "def get_weight_updates(w1,w0,X,y,learning_rate = 0.01) :\n",
        "  N = len(y)\n",
        "  w1_update = np.zeros_like(w1)\n",
        "  w0_update = np.zeros_like(w0)\n",
        "  y_pred = np.dot(X,w1.T) + w0\n",
        "  diff = y-y_pred\n",
        "  \n",
        "  #모두 1값을 가진 행렬 생성\n",
        "  w0_factors = np.ones((N,1))\n",
        "\n",
        "  #업데이트 계산\n",
        "  w1_update = -(2/N)*learning_rate*(np.dot(X.T,diff))\n",
        "  w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T,diff))\n",
        "\n",
        "  return w1_update,w0_update"
      ],
      "metadata": {
        "id": "oW1ACNaSN1mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용\n",
        "def gradient_descent_steps (X,y,iters = 10000) :\n",
        "  w0 = np.zeros((1,1))\n",
        "  w1 = np.zeros((1,1))\n",
        "\n",
        "  for ind in range(iters) :\n",
        "    w1_update, w0_update = get_weight_updates(w1,w0,X,y,learning_rate = 0.01)\n",
        "    w1 = w1 - w1_update\n",
        "    w0 = w0 - w0_update\n",
        "  \n",
        "  return w1,w0"
      ],
      "metadata": {
        "id": "DIuI_69PN2Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost(y,y_pred) :\n",
        "  N = len(y)\n",
        "  cost = np.sum(np.square(y-y_pred))/N\n",
        "  return cost\n",
        "\n",
        "w1,w0 = gradient_descent_steps(X,y,iters = 100)\n",
        "print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0,0],w0[0,0]))\n",
        "y_pred = w1[0,0] * X + w0\n",
        "print('Gradient Descent Total Cost : {0:.4f}'.format(get_cost(y,y_pred)))"
      ],
      "metadata": {
        "id": "oSecU9ihN3r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X,y)\n",
        "plt.plot(X,y_pred)"
      ],
      "metadata": {
        "id": "YoYDfLTDN5yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(미니배치) 확률적 경사 하강법\n",
        "\n",
        "def stochastic_gradient_descent_steps (X,y, batch_size=10, iters = 1000) :\n",
        "  w0 = np.zeros((1,1))\n",
        "  w1 = np.zeros((1,1))\n",
        "  prev_cost = 100000\n",
        "  iter_index = 0\n",
        "\n",
        "  for ind in range(iters) :\n",
        "    np.random.seed(ind)\n",
        "    stochastic_random_index = np.random.permutation(X.shape[0])\n",
        "    sample_X = X[stochastic_random_index[0:batch_size]]\n",
        "    sample_y = y[stochastic_random_index[0:batch_size]]\n",
        "    w1_update,w0_update = get_weight_updates(w1,w0,sample_X,sample_y,learning_rate = 0.01)\n",
        "    w1 = w1 - w1_update\n",
        "    w0 = w0 - w0_update\n",
        "  \n",
        "  return w1,w0"
      ],
      "metadata": {
        "id": "NFg3_tICU7OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1,w0 = stochastic_gradient_descent_steps(X,y,iters = 1000)\n",
        "print('w1 :',round(w1[0,0],3), 'w0:',round(w0[0,0],3))\n",
        "y_pred = w1[0,0]*X + w0\n",
        "print('Stochastic Gradient Descent Total Cost : {0:.4f}'.format(get_cost(y,y_pred)))"
      ],
      "metadata": {
        "id": "FtQ30OWWVC4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 다항 회귀와 과(대)적합/ 과소적합 이해"
      ],
      "metadata": {
        "id": "1Qsa5lh5VGT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 다항회귀 이해 -> **선형회귀**\n",
        "\n",
        ": 2차, 3차 방정식과 같은 다항식으로 표현되는 것\n",
        "\n",
        "$$ y = w_0 + w_1*x_1 + w_2*x_2 + w_3*x_1*x_2 + w_4*x_1^2 + w_5*x_2^2 $$\n",
        "\n",
        "* `PolynomialFeatures` degree 파라미터를 통해 입력 받은 단항식 피처를 degree에 해당하는 다항식 피처로 변환"
      ],
      "metadata": {
        "id": "wbxoK3m4VJaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "X = np.arange(4).reshape(2,2)\n",
        "print('일차 단항식 계수 피처 :\\n',X)\n",
        "\n",
        "#degree = 2인 2차 다항식으로 변환\n",
        "poly = PolynomialFeatures(degree = 2)\n",
        "poly.fit(X)\n",
        "poly_ftr = poly.transform(X)\n",
        "print('변환된 2차 다항식 계수 피처 : \\n',poly_ftr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFCC_yEoVDbd",
        "outputId": "a5f68b29-0007-4e65-c6c0-bfdc4b3ef1bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일차 단항식 계수 피처 :\n",
            " [[0 1]\n",
            " [2 3]]\n",
            "변환된 2차 다항식 계수 피처 : \n",
            " [[1. 0. 1. 0. 0. 1.]\n",
            " [1. 2. 3. 4. 6. 9.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처값을 입력하면 결정값을 반환하는 함수\n",
        "\n",
        "def polynomial_fun(X) :\n",
        "  y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n",
        "  return y\n",
        "\n",
        "X = np.arange(4).reshape(2,2)\n",
        "print('일차 단항식 계수 feature : \\n',X)\n",
        "y = polynomial_fun(X)\n",
        "print('삼차 다항식 결정값 :\\n',y)\n"
      ],
      "metadata": {
        "id": "96uO1r-gVPGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3차 다항식 변환\n",
        "poly_ftr = PolynomialFeatures(degree = 3).fit_transform(X)\n",
        "print('3차 다항식 계수 feature : \\n',poly_ftr)\n",
        "\n",
        "#Linear Regression에 3차 다항식 계수 featue와 3차 다항식 결정값으로 학습 후 회귀 계수 확인\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(poly_ftr,y)\n",
        "print('Polynomial 회귀 계수 \\n', np.round(model.coef_,2))\n",
        "print('Polynomial 회귀 Shape :', model.coef_.shape)"
      ],
      "metadata": {
        "id": "otwmTaNxVS8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "def polynomial_func(X) :\n",
        "  y = 1+ 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n",
        "  return y\n",
        "\n",
        "model = Pipeline([('poly',PolynomialFeatures(degree = 3)),\n",
        "                  ('linear',LinearRegression())])\n",
        "X = np.arange(4).reshape(2,2)\n",
        "y = polynomial_func(X)\n",
        "\n",
        "model = model.fit(X,y)\n",
        "print('Polynomial 회귀 계수 \\n', np.round(model.named_steps['linear'].coef_,2))"
      ],
      "metadata": {
        "id": "KJhlJC24VVhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 다항회귀를 이용한 과소적합 및 과적합 이해"
      ],
      "metadata": {
        "id": "JyrPa496Vat5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "%matplotlib inline\n",
        "\n",
        "#X값에 대해 코사인 변환 값 반환\n",
        "def true_fun(X) :\n",
        "  return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "n_samples = 30\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n"
      ],
      "metadata": {
        "id": "UZJGLSnvVYJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (14,5))\n",
        "degrees = [1,4,15]\n",
        "\n",
        "#다항 회귀 차수를 변화시키면서 비교\n",
        "for i in range(len(degrees)) :\n",
        "  ax = plt.subplot(1,len(degrees),i + 1)\n",
        "  plt.setp(ax,xticks = (),yticks = ())\n",
        "\n",
        "  polynomial_features = PolynomialFeatures(degree = degrees[i], include_bias = False)\n",
        "  linear_regression = LinearRegression()\n",
        "  pipeline = Pipeline([('polynomial_features',polynomial_features),\n",
        "                       ('linear_regression',linear_regression)])\n",
        "  pipeline.fit(X.reshape(-1,1),y)\n",
        "\n",
        "  #교차 검증으로 다항회귀 평가\n",
        "  scores = cross_val_score(pipeline,X.reshape(-1,1),y,scoring = 'neg_mean_squared_error',cv = 10)\n",
        "  coefficients = pipeline.named_steps['linear_regression'].coef_\n",
        "  print('\\nDegree {0} 회귀 계수는 {1}입니다.'.format(degrees[i],np.round(coefficients,2)))\n",
        "  print('Degree {0} MSE는 {1} 입니다.'.format(degrees[i],-1*np.mean(scores)))\n",
        "\n",
        "  X_test = np.linspace(0,1,100)\n",
        "  #예측값 곡선\n",
        "  plt.plot(X_test,pipeline.predict(X_test[:,np.newaxis]), label = 'Model')\n",
        "  #실제 값 곡선\n",
        "  plt.plot(X_test, true_fun(X_test), '--',label = 'True function')\n",
        "  plt.scatter(X,y,edgecolor = 'b',s = 20, label = 'Samples')\n",
        "\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.xlim((0,1))\n",
        "  plt.ylim((-2,2))\n",
        "  plt.legend(loc = 'best')\n",
        "  plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(degrees[i],-scores.mean(),scores.std()))"
      ],
      "metadata": {
        "id": "SEM8AMJ2VdU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Degree 1의 예측 곡선은 단순 선형회귀와 같음. 예측 곡선이 학습 데이터의 패턴을 제대로 반영하지 못하고 있는 **과소적합 모델**\n",
        "\n",
        "* Degree 4 의 예측 곡선은 실제 데이터와 유사한 모습.\n",
        "\n",
        "* Degree 15의 예측 곡선은 예측 곡선이 학습 데이터 세트만 정확히 예측하고, 테스트 값의 실제 곡선과는 완전히 다른 형태로 **과적합 모델**"
      ],
      "metadata": {
        "id": "q7pK5mmRViDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) 편향 분산 트레이드 오프(Bias-Variance trade off)\n",
        "\n",
        "* Degree 1과 같은 모델은 단순화된 모델로서 지나치게 한 방향으로 치우친 경향 -> `고편향(High Bias)`\n",
        "* Degree 15와 같은 모델은 학습 데이터 하나하나의 특성을 반영하여 매우 복잡하고 지나치게 높은 변동성 -> `고분산(High Variance)`\n",
        "* 일반적으로 편향과 분산은 **한 쪽이 높으면 한 쪽이 낮아지는 경향**\n",
        "* 높은 편향/낮은 분산 : **과소적합** \n",
        "* 낮은 편향/높은 분산 : **과적합**"
      ],
      "metadata": {
        "id": "-YDNOamiVmKq"
      }
    }
  ]
}
